{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9c5009",
   "metadata": {},
   "source": [
    "# Ableton LLM Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8132d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8294f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51cc72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import live\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5df65",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0eacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_CLASSIFIER_MODEL_NAME = \"ableton_osc_matcher\"\n",
    "TEXT_CLASSIFIER_MODEL_PATH = f\"../artifacts/{TEXT_CLASSIFIER_MODEL_NAME}\"\n",
    "# DEVICE = (\n",
    "#     \"mps\"\n",
    "#     if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "INPUT_CHANNELS = 1\n",
    "SAMPLE_RATE = 16_000  # Whisper sample rate\n",
    "MAX_FRAMES = 5 * SAMPLE_RATE  # Max recording time\n",
    "\n",
    "sd.default.samplerate = SAMPLE_RATE\n",
    "sd.default.channels = INPUT_CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4211269",
   "metadata": {},
   "source": [
    "## Voice recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4492a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_recording() -> np.ndarray:\n",
    "    return np.zeros((MAX_FRAMES, INPUT_CHANNELS))\n",
    "\n",
    "def start_recording() -> np.ndarray:\n",
    "    recording = init_recording()\n",
    "    sd.rec(out=recording)\n",
    "    return recording\n",
    "\n",
    "def trim_recording(recording: np.ndarray) -> np.ndarray:\n",
    "    last_non_zero = np.max(np.where(recording.any(axis=1))[0]) + 1\n",
    "    return recording[:last_non_zero]\n",
    "\n",
    "def stop_recording(out: np.ndarray) -> np.ndarray:\n",
    "    sd.stop()\n",
    "    return trim_recording(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b384cbd",
   "metadata": {},
   "source": [
    "## Voice transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801d750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "speech_recognition_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=speech_recognition_model, tokenizer=processor.tokenizer, feature_extractor=processor.feature_extractor)\n",
    "\n",
    "def transcribe(recording: np.ndarray) -> str:\n",
    "    result = transcriber(recording.squeeze())[\"text\"].strip()\n",
    "    print(f\"[Transcriber]: {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932d7a2",
   "metadata": {},
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798371d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier_model = AutoModelForSequenceClassification.from_pretrained(TEXT_CLASSIFIER_MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_CLASSIFIER_MODEL_PATH)\n",
    "classifier = pipeline(\"text-classification\", model=text_classifier_model, tokenizer=tokenizer)\n",
    "\n",
    "def classify_text(command: str) -> str:\n",
    "    results: list[dict] = classifier(command)\n",
    "    result = results[0]\n",
    "    print(f\"[Classifier]: {result}\")\n",
    "    if result[\"score\"] < 0.3:\n",
    "        raise Exception(\"Low confidence\")\n",
    "    return result[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93db2d",
   "metadata": {},
   "source": [
    "## Ableton Live connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06fada9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Live]: ok\n"
     ]
    }
   ],
   "source": [
    "livequery = live.Query()\n",
    "test_response = livequery.query(\"/live/test\")\n",
    "print(f\"[Live]: {test_response[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e910dd",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34b4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(recording: np.ndarray) -> None:\n",
    "    command = transcribe(recording)\n",
    "    osc_endpoint = classify_text(command)\n",
    "    live_result = livequery.cmd(osc_endpoint)\n",
    "    print(f\"[Live]: {live_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be251e1e",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1140d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b2737c61b45f18251ff993d7c70d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='Record', icon='microphone', style=ButtonStyle(), tooltip='Record')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording = init_recording()\n",
    "\n",
    "button = widgets.Button(\n",
    "    description=\"Record\",\n",
    "    disabled=False,\n",
    "    button_style=\"danger\",\n",
    "    tooltip=\"Record\",\n",
    "    icon=\"microphone\",\n",
    ")\n",
    "\n",
    "def on_click(b: widgets.Button) -> None:\n",
    "    global recording\n",
    "    if b.description == \"Record\":\n",
    "        b.description = \"Done\"\n",
    "        b.tooltip = \"Done\"\n",
    "        b.button_style = \"warning\"\n",
    "        b.icon = \"microphone-slash\"\n",
    "        recording = start_recording()\n",
    "    else:\n",
    "        b.description = \"Record\"\n",
    "        b.tooltip = \"Record\"\n",
    "        b.button_style = \"danger\"\n",
    "        b.icon = \"microphone\"\n",
    "        b.disabled = True\n",
    "        recording = stop_recording(recording)\n",
    "        run(recording)\n",
    "        b.disabled = False\n",
    "\n",
    "button.on_click(on_click)\n",
    "button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc7e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
